{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 2 - Model training with Logistic Regression\n",
    "<!-- ![linear-vs-logistic-regression--medium](https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg) -->\n",
    "\n",
    "Welcome to your second lab! This notebook contains all the code and comments that you will need to submit. Labs are running over two weeks and the places where you need to edit are highlighted in red. Please note that the colour highlighting might not work across all IDEs, so make sure you check and all cells! </br>\n",
    "Feel free to add in your own markdown for additional comments, and also directly comment your code.\n",
    "\n",
    "__Submission details:__ \n",
    "- __Make sure you have run all your cells from top to bottom (you can click _Kernel_ and _Restart Kernel and Run All Cells_).__ </br>\n",
    "- __Submit this Jupyter Notebook (_Lab_2.ipynb_) and also submit the .py file that is generated.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Your Python version is 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "## This code snippet does not need to be edited\n",
    "\n",
    "from python_environment_check import check_packages\n",
    "from python_environment_check import set_background\n",
    "\n",
    "## Colour schemes for setting background colour\n",
    "white_bgd = 'rgba(0,0,0,0)'\n",
    "red_bgd = 'rgba(255,0,0,0.2)'\n",
    "blue_bgd = 'rgba(0,0,255,0.2)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "## Code snippets in red (similar to this) is where you need to edit your answer)\n",
    "## Note: Whether the colour will actually be displayed will depend on your environment (it should work on jupyterlab, but might not on Google Colab),\n",
    "##       so make sure you check all the cells independent of their colour!\n",
    "\n",
    "# Set your student ID and name here:\n",
    "student_number = 12345678  # 12345678\n",
    "student_name = \"John Doe\" # \"John Doe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preliminaries \n",
    "Before we start with the actual lab, let's run a quick version check and see if you're environment is suited / set up as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checks for the minimum version requirements for a few libraries\n",
    "## Credits to: https://sebastianraschka.com/blog/2022/ml-pytorch-book.html for the code related to checking version requirements\n",
    "\n",
    "\n",
    "d = {\n",
    "    #'torch': '1.8.0',\n",
    "    #'torchvision': '0.9.0',\n",
    "    'numpy': '1.21.2',\n",
    "    'matplotlib': '3.4.3',\n",
    "}\n",
    "\n",
    "check_packages(d)\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've learnt last time, libraries are important as they save us time. We use the `import` function to import libraries, and use _as_ to rename the library within our workspace. For example, we could use _import numpy_ by itself and then reference Numpy functions by _numpy.function_. However, we can also keep things simple and define an alias of Numpy as _np_, followed by referencing the functions as _np.function_. Note that using _np_ is a common naming convention for numpy, and you will likely come across this in many other codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries, you do not need to import any additional libraries for this lab\n",
    "\n",
    "import numpy as np ## Numpy is the fundamental building block of understanding tensor (matrices) within Python\n",
    "import matplotlib.pyplot as plt ## Matplotlib.pyplot is the graphing library that we will be using throughout the semester\n",
    "import random ## Useful for sampling \n",
    "# import sys ## Useful to retrieve some system information\n",
    "\n",
    "# from scipy.special import gamma ## Pre-built gamma function that we will use for this lab (for the final task)\n",
    "# import math # Basic math library\n",
    "\n",
    "import os ## Useful for running command line within python\n",
    "from IPython.display import Image ## For markdown purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided some numerical answers for you to aim for. To replicate these results, do not change any of the hyper parameters that are not does not have __set_background(red_bgd)__ in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - A quick recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture that the name **Logistic Regression** is slightly misleading: Instead of regression, we are solving **classification** problems with this appraoch! </br>\n",
    "More specifically, we model the probabilities of the outcomes of our classification problem, and we can thus understand logistic regression like an extension of linear regression to solve classification (_i.e._, for categorical data). \n",
    "\n",
    "A visual comparison of Linear Regression vs. Logistic regression and especially their 'output' predictions $Y$ for different input points $X$ in a simple 1D case is depicted in the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this lab, there will be code and written answers that you need to fill in / complete. The comments in the code snippet and markdown text will guide you on what you need to do. </br>\n",
    "\n",
    "**Note that you can find examples of the expected outcomes within the lab instructions (.pdf document)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 1 - The sigmoid function and making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will be writing the first parts of your code that is essential to predict the outcome of a logistic regression problem. </br>\n",
    "In detail, you are going to\n",
    "- 1.1 Implement and visualise the **sigmoid function**\n",
    "- 1.2 Write code to **predict the outcome** of a classification problem using a pre-trained logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1  The sigmoid function\n",
    "\n",
    "The '_sigmoid function_' $\\sigma$, sometimes also called '_logistic function_', is a mathematical function that shows a characteristic \"S\"-shaped curve as you've seen during the lecture (hence its name!). We commonly use this function in our logistic regression to map the regression outputs to a range from 0 to 1. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computing the sigmoid\n",
    "In this task, you are now first asked to write a function that computes the output of the sigmoid function $\\sigma(\\boldsymbol{x})$ for any input value $\\boldsymbol{x}$. </br>\n",
    "_Hint:_ Use the _numpy_ library you have been introduced to in the previous lab to allow easy computation of multi-dimensional input values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "# Implement the sigmoid function\n",
    "def sigmoid(x):\n",
    "    \n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the correctness of your implementation, call your sigmoid function with the following input values:\n",
    "- $x_1 = 2.$ \n",
    "- $\\boldsymbol{x}_2 = [5., 10.]$\n",
    "- $\\boldsymbol{x}_3 = [ [-5., -7.], [4., 2.] ]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Define the inputs x1, x2 and x3 (hint: use numpy for arrays):\n",
    "x_1 = ???\n",
    "x_2 = ???\n",
    "x_3 = ???\n",
    "\n",
    "# Use your implemented sigmoid function to obtain the results for the given x_i\n",
    "z_1 = ???\n",
    "z_2 = ???\n",
    "z_3 = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's print the obtained results\n",
    "\n",
    "print(f'z_1: {z_1}')\n",
    "print(f'z_2: {z_2}')\n",
    "print(f'z_3: {z_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should obtain a printed output similar to this: </br>\n",
    "- z_1: &nbsp;&nbsp;0.8807970779778823 </br>\n",
    "- z_2: [0.99330715 0.9999546 ] </br>\n",
    "- z_3: [[6.69285092e-03 9.11051194e-04]</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "[9.82013790e-01 8.80797078e-01]]</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualising the sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your experience with numpy and matplotlib from the previous lab to visualise the output range of our implemented sigmoid function for a 1 dimensional case in the range $x \\in [-10, 10]$. </br>\n",
    "In detail, we ask you to:\n",
    "- Visualise the outputs of the sigmoid as a **line plot in blue colour**.\n",
    "- Visualise the outputs of the sigmoid as a **scatter plot in red colour**.\n",
    "\n",
    "Please plot both into the same figure! </br>\n",
    "Use **50 data points** to get a smooth plot, and make sure to add an appropriate **plot title** and to **label the axes**!</br>\n",
    "_Hint_: Check the matplotlib docu for details how to do this. You can find many examples there for a variety of different applications. (Check the lab instructions .pdf to see an example of what is expected here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Visualise the output of the sigmoid function in a range from -10 to 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# What is the difference between the line plot and the scatter plot?\n",
    "# What would happen if you only used a small number of datapoints to plot? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Making predictions\n",
    "You will now use your implemented sigmoid function to solve an actual classification problem using logistic regression. </br>\n",
    "As discussed in the lecture, a prediction $\\hat{y}$ can be obtained by using our logistic regression model via $\\hat{y}=\\sigma(\\boldsymbol{w}^\\top \\boldsymbol{x})$\n",
    "\n",
    "Note that for this example, we want to be able to use many samples at the same time - all of which are stored in one single vector $X$, which is similar to the test case $\\boldsymbol{x_3}$ from before.\n",
    "\n",
    "Also note that we predict the distribution over the classes, _i.e._ the probablity for each class -> to get the 'hard' class label, we will later on assign everything below the probablity of 0.5 to class1 and above to class2 (boundary could be included in either)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# Read in the lab2_main_data.npz using numpy --> data has been saved via np.savez (check docu for more details)\n",
    "\n",
    "# Components can be accessed like a dictionary after the file has been loaded, and the file contains the following:\n",
    "# 'X_train' : training data we're going to use\n",
    "# 'y_train' : labels for the training data\n",
    "# 'X_test'  : test data we're going to use for evaluation, but NOT for training\n",
    "# 'y_test'  : labels for the test data\n",
    "# 'w_pret'  : a set of pretrained weights for the logistic regression model\n",
    "\n",
    "loaded_data = ???\n",
    "X_train = ???\n",
    "y_train = ???\n",
    "X_test = ???\n",
    "y_test = ???\n",
    "w_pret = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# Add side by side plots here to visualise your train and test data (Use subplots). \n",
    "# This is the two classes you will be classifying. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the shape of the data!\n",
    "# Note that we assume certain shapes of data for the basic logistic regression formulas to work, \n",
    "# so make sure you understand which elements should be multiplied with each other!\n",
    "# Hint: In case the data is stored in a different shape, you can easily transpose the matrices!\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_test: {y_test.shape}')\n",
    "print(f'w_pret: {w_pret.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Explain the shapes of the data: \n",
    "#  What do the numbers represent, and why do they differ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting class probabilities via logistic regression\n",
    "After having obtained the data and a set of pretrained weights for our logistic regression model, you are now going to\n",
    "- Implement a function to predict outcomes using a linear regression model (taking in data $\\boldsymbol{X}$ and parameters $\\boldsymbol{w}$)\n",
    "- Test your function on the example data provided below\n",
    "- Test your function on the read-in training data $\\boldsymbol{X}_{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# # Write a prediction function -> We predict the output class probability, NOT the class label (no 0,1 rounding)\n",
    "# def predict(X, w):\n",
    "#     # Reshape X input to have data in the columns [mxn]-->[nxm]\n",
    "#     # Perfrom Matrix multiplication\n",
    "#     # Calling Sigmoid\n",
    "\n",
    "#     return ??? #output y_hat as an mx1 array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your prediction function using the following toy data points / samples:\n",
    "- $\\boldsymbol{X}_1 = [[0.5, 0.1]]$\n",
    "- $\\boldsymbol{X}_2 = [ [-0.5, -0.7], [0.4, 0.2] ]$\n",
    "- $\\boldsymbol{X}_3 = [ [-0.3, -0.15], [0.89, -0.02], [-0.35, 0.01], [0.26, -0.64] ]$\n",
    "\n",
    "Note that our data is stored as [number of samples, dim], so you need to pay attention to possibly required transpose operations to perform the calculations correctly. </br>\n",
    "We also want the same to be true for our predictions, _i.e._ return them in the format [number of samples, 1] to match the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "## Define the toy input data\n",
    "X_1 = ??? \n",
    "X_2 = ???\n",
    "X_3 = ???\n",
    "\n",
    "## Obtain predictions using predict function and pretrained parameters w_pret\n",
    "y_hat_1 = predict(X_1, w_pret)\n",
    "y_hat_2 = predict(X_2, w_pret)\n",
    "y_hat_3 = predict(X_3, w_pret)\n",
    "\n",
    "print(f'y_hat_1: {y_hat_1}')\n",
    "print(f'y_hat_2: {y_hat_2}')\n",
    "print(f'y_hat_3: {y_hat_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If your predict function works as intended, your results should be close to:** </br>\n",
    "y_hat_1: &nbsp;[[0.51370692]]</br>\n",
    "y_hat_2: [[0.61837619]</br>\n",
    "&emsp;&emsp;&emsp;&emsp; &nbsp;&nbsp;[0.48409849]]</br>\n",
    "y_hat_3: [[0.51192789]</br>\n",
    "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.56831845]</br>\n",
    "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.47251372]</br>\n",
    "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.65665833]]</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's additionally test the predict function on the $\\boldsymbol{X}_{train}$ data we read in from the stored file and compare the output shape to the shape of the provided labels $\\boldsymbol{y}_{train}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Test on the read-in X_train data:\n",
    "y_hat_data = ??? \n",
    "\n",
    "# Compare shapes:\n",
    "print(y_hat_data.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# check first predicted element\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your predict function works correctly, the shapes of the predictions and the provided labels should match. </br>\n",
    "You can also take a look at the first element of the prediction - it should have a value of around $0.98278992$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 - Training a model via Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second task, you will be writing code for the essential components to **train your own logistic model via Gradient Descent** given some training data. </br>\n",
    "In detail, you are going to\n",
    "- 2.1 Implement a function that computes and returns **gradient and cost** of the logistic regression\n",
    "- 2.2 Write code to perform the actual **gradient descent algorithm** for a fixed number of iterations and **train your own logistic regression model** given some training data\n",
    "- 2.3 **Evaluate your model** on previously unseen test data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in more detail during the lecture, we commonly use the so-called _Cross Entropy_ Loss to calculate the cost of our logistic regression problem. This loss function can be defined as </br>\n",
    "</br>\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_{\\mathrm{CE}}(\\boldsymbol{w})= - \\frac{1}{m}\\sum_{i=1}^{m} \\Big\\lbrace y_i \\log \\Big(\\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i}\\Big) + \\left( 1 - y_i \\right) \\log \\Big( 1- \\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i}\\Big) \\Big\\rbrace\n",
    "\\end{equation}\n",
    "</br>\n",
    "In this notation, $\\sigma(z) = 1 / (1 + \\exp(-z))$ denotes the **sigmoid** function, and $(\\boldsymbol{x}_1,y_1),(\\boldsymbol{x}_2,y_2),\\dots,(\\boldsymbol{x}_m,y_m)$ with $\\boldsymbol{x}_i \\in \\mathbb{R}^n, y_i \\in \\lbrace 0, 1\\rbrace$ represent the $m$ training samples (with labels $y_i$). </br>\n",
    "The gradient of the cross entropy loss w.r.t. the weights $\\boldsymbol{w}$ can be written as \n",
    "</br>\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}} = \\frac{1}{m}\\sum_{i=1}^{m} \\Big(\\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i} - y_i \\Big) \\boldsymbol{x}_i\n",
    "\\end{equation}\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1  Gradient and Cost Computation\n",
    "In this part, we want to define a function that is able to compute our cross-entropy loss $\\mathcal{L}_{\\mathrm{CE}}$, as well as the gradient $\\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}}$ of our loss $\\mathcal{L}_{\\mathrm{CE}}$ _w.r.t._ the parameters $\\boldsymbol{w}$. </br>\n",
    "As you can see above, all we need to compute the gradient vector is the prediction of the model $\\hat{y}$ and the actual labels $y$, as well as the input data points $\\boldsymbol{X}$. The loss itself is even more simple and only requires the predictions $\\hat{y}$ and true labels $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "def compute_loss_and_grad(X, y, y_hat):\n",
    "    # Inputs:\n",
    "    #    Set of samples X (each sample is a row in X),\n",
    "    #    Corresponding ground-truth labels y \n",
    "    #    Predicted class probabilities y_hat\n",
    "    \n",
    "    # Import smallest number represented to handle log(0) edge case\n",
    "    eps = 1e-12    \n",
    "    # Compute the mean cross-entropy loss w.r.t. the parameters w (mean as defined in lecture)\n",
    "    loss = ???\n",
    "    # log(0) might throw error, so handled via small eps -> Might be irrelevant, to be tested\n",
    "    \n",
    "    # Compute the gradient vector (mean over all samples as defined in lecture)\n",
    "    grad_vec = ???\n",
    "    \n",
    "    # Return loss and gradient vector\n",
    "    return loss, grad_vec.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training with Gradient Descent\n",
    "\n",
    "Gradient descent, sometimes also referred to as _steepest descent_, is a popular first-order iterative optimisation method that has become ubiquitous in the machine and deep learning context. As you have heard in the lecture, the idea is to find the local minimum of a differentiable function by repeatedly taking steps in the opposite direction of the gradient of the function at the current point - i.e. in the direction of its steepest descent.\n",
    "\n",
    "In this section of the lab, you are going to implement the **Gradient Descent algorithm** as a function that we can use afterwards to train our logistic regression model!\n",
    "\n",
    "The main parts of the algorithm work as follows:\n",
    "- Initialise hyperparameters like step-size aka learning rate, and number of iterations\n",
    "- Randomly initialise the set of parameters $\\boldsymbol{w}_{init}$ that shall be optimised\n",
    "- For a certain number of iterations, do: \n",
    "    - Obtain the prediction using the current weights $\\boldsymbol{w}_i$ and training data $\\boldsymbol{X}_{train}$\n",
    "    - Compute the loss $\\mathcal{L}_{\\mathrm{CE}}$ and the gradient vector $\\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}}$ w.r.t. the current parameters $\\boldsymbol{w}_i$\n",
    "    - Update the parameters using the gradient vector and learning rate _lr_\n",
    "- After all iterations are finished, return the final optimised set of parameters\n",
    "\n",
    "In addition, we ask you to also:\n",
    "- Return a list of all losses (one value for each iteration)\n",
    "- Return a list of all gradient vectors (one vector for each iteration)\n",
    "- Implement an option via the argument \"logging\" to switch on printing a string containing the 'iteration' and the 'loss' for each iteration\n",
    "\n",
    "Note that the initial set of parameters $\\boldsymbol{w}_{init}$, the hyperparameters as well as the training data $\\boldsymbol{X}_{train}$ and labels $\\boldsymbol{y}_{train}$ are passed as input arguments to your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting some hyperparameters: \n",
    "lr = 0.5         # Learning rate\n",
    "num_epochs = 20    # Number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "def gradient_descent(w_init, num_epochs, lr, X_train, y_train, logging=False):\n",
    "    ## Create empty lists to store the values for loss and gradient vector over all \n",
    "    #  'num_epochs' iterations of our gradient descent optimisation procedure\n",
    "    losses = []\n",
    "    grad_vecs = []\n",
    "    \n",
    "    # Init the parameters\n",
    "    w = w_init\n",
    "\n",
    "    ## Implement the actual gradient descent using the previously implemented functions\n",
    "    for ep in range(num_epochs):\n",
    "        # Compute prediction using current weights\n",
    "        preds = ??? \n",
    "        # Compute loss and gradient vector for current prediction\n",
    "        loss, grad_vec = ??? \n",
    "        \n",
    "        # update the weight parameter according to the GD here\n",
    "        w = ??? \n",
    "        \n",
    "        losses.append(loss)\n",
    "        grad_vecs.append(grad_vec)\n",
    "        \n",
    "        if logging:\n",
    "            print(f'Ep {ep+1:2d} | Loss: {loss:.3f}')\n",
    "\n",
    "    return w, losses, grad_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the function on the training set\n",
    "\n",
    "## Start from a random initialisation\n",
    "np.random.seed(12345)\n",
    "w_init = np.random.randn(X_train.shape[1],1) \n",
    "\n",
    "# Obtain the final weights via gradient descent\n",
    "w_final, _, _ = gradient_descent(w_init, num_epochs, lr, X_train, y_train, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluating the trained model\n",
    "\n",
    "After you have obtained your optimised set of parameters $\\boldsymbol{w}^{*}$, let's see how your model performs! </br>\n",
    "\n",
    "To this end, you are going to:\n",
    "- Obtain the predictions (class probabilities) $\\hat{y}_{train}$ for the training data $\\boldsymbol{X}_{train}$ using $\\boldsymbol{w}^{*}$\n",
    "- Obtain the predictions (class probabilities) $\\hat{y}_{test}$ for the test data $\\boldsymbol{X}_{test}$ using $\\boldsymbol{w}^{*}$\n",
    "- Convert these into the actual predicted labels (everything with probability >=0.5 is more likely to be of class 1 and thus gets label '1' assigned ; below gets label '0')\n",
    "- Count how many samples have been correctly classified and compute the percentage (_i.e._, the accuracy in %)\n",
    "- Report your obtained accuracies for both training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Evaluate the obtained model on training data and previously unseen test data\n",
    "\n",
    "# Obtain predicted class probabilities for train and test data\n",
    "y_hat_train = ??? \n",
    "y_hat_test  = ??? \n",
    "\n",
    "# Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
    "c_hat_train = ??? \n",
    "c_hat_test = ??? \n",
    "\n",
    "# Evaluate the classification accuracy for training and test data\n",
    "acc_train = ??? \n",
    "acc_test = ??? \n",
    "\n",
    "# Print outputs\n",
    "print(f'Training accuracy: {acc_train:.3f} | Test accuracy: {acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation work correctly and using the provided hyperparameter settings, you should obtain something around: \n",
    "\n",
    "Training accuracy: 0.868 | Test accuracy: 0.853"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 - Analysing convergence and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Improving the accuracy\n",
    "Our previous choice of hyperparameters might not be the best possible one (or even close to it). </br>\n",
    "Can you achieve a **better test accuracy** by changing the hyperparameters from the previous task? </br>\n",
    "Try to improve upon the standard choice by varying the learning rate `lr` and number of training iterations `num_epochs`. Report your choice and best results below!\n",
    "\n",
    "**Potentially also let them analyse training vs test accuracy? Would it overfit on this data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Report your best results as well as hyperparameter choices!\n",
    "???\n",
    "\n",
    "# Could basically be anything that gets better results, some good combination of learning rate \n",
    "# and probably slightly higher number of epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are now going to take a closer look at how gradient descent 'progresses' for different choices of hyperparameters.\n",
    "\n",
    "Given the provided set of learning rates _lrs_, run your implemented gradient descent method and plot the obtained loss values over the number of iterations for each learning rate.  </br>\n",
    "Additionally save the training and test accuracies achieved for each learning rate. </br>\n",
    "You can re-use/copy-and-paste your code from above, or define it as a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provided list of learning rates to train on:\n",
    "lrs = [0.05, 0.1, 0.5, 1.0, 2.5, 5., 10., 75.]\n",
    "# Max number of iterations for GD algorithm to run\n",
    "num_epochs = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Run gradient descent for all learning rates, and plot results\n",
    "fig, ax = plt.subplots()\n",
    "x_vals = range(num_epochs)\n",
    "leg = []  # legend entries\n",
    "w_finals = {}\n",
    "for lr in lrs:\n",
    "    ## Start from a random initialisation\n",
    "    w_init = np.random.randn(X_train.shape[1],1) \n",
    "    ## Start from a random initialisation\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    # Obtain the final weights via gradient descent\n",
    "    w_final, losses, _ = ???\n",
    "    ax.plot(x_vals, losses)\n",
    "    leg.append(f'lr = {lr}')\n",
    "    w_finals[f'lr={lr}'] = w_final\n",
    "\n",
    "# Add legend, title and label the axes\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Evaluating the stored parameter sets to retrieve train and test accuracies\n",
    "def evaluate(X,y,w):\n",
    "    # Obtain predicted class probabilities\n",
    "    y_hat = ???\n",
    "\n",
    "    # Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
    "    c_hat = ???\n",
    "\n",
    "    # Evaluate the classification accuracy\n",
    "    acc = ???\n",
    "    \n",
    "    return acc\n",
    "\n",
    "print(' >>> Training accuracies for different learning rates: <<<')\n",
    "for k,v in w_finals.items():\n",
    "    print(f'{k}: \\t {round(evaluate(X_train, y_train, v),3)}')\n",
    "    \n",
    "print('\\n >>> Test accuracies for different learning rates: <<<')\n",
    "for k,v in w_finals.items():\n",
    "    print(f'{k}: \\t {round(evaluate(X_test, y_test, v), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Describe & Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions and elaborate on your observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "## Answer the following questions:\n",
    "\n",
    "# What do you observe? Are there general trends in convergence visible, and are they ‘good’ or ‘bad’? \n",
    "\n",
    "# What do you think would be the best choice out of the set of provided learning rates, and why? \n",
    "\n",
    "# Can you find an even better one?\n",
    "\n",
    "# How is the accuracy on the training and test data related to these convergence results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 - Using non-linear features for better classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will be writing code to **train your own logistic model via Gradient Descent** given some training data with non-linear features. </br>\n",
    "In detail, you are going to\n",
    "- 4.1 Visualize dataset and create non-linear train and test datasets.\n",
    "- - Scenario 1: $\\mathbb{R}^2 \\ni x = (x_1,x_2)^\\top$ \n",
    "- - Scenario 2: $\\mathbb{R}^i \\ni x = ? $, suggest the mapping and value for **i** considering data distribution.\n",
    "- 4.2 Using above functions (predict, compute_loss_and_grad), **train your own logistic regression model** given some training data on scenario 1. **Evaluate your model** on previously unseen test data points.\n",
    "- 4.3 **Plot the decision boundary** for scenario 1 on test data.\n",
    "- 4.4 Using above functions (predict, compute_loss_and_grad), **train your own logistic regression model** given some training data on scenario 2. **Evaluate your model** on previously unseen test data points.\n",
    "- 4.5 **Plot the decision boundary** for scenario 2 on test data.\n",
    "- 4.6 Evaluate model's performance adapting **Decaying learning rate** during training for scenario 2. Report your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load data and create train and test dataset\n",
    "\n",
    "You are going to start the task 4 by loading numpy data and creating train and test datasets. To get an idea about the data distribution, visualize train and test data using matplotlib. Then map $\\mathbb{R}^2$ to $\\mathbb{R}^i$ and create anothor train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# Load numpy dataset from Lab2_task4_data.npz\n",
    "loaded_data_task4 = ???\n",
    "\n",
    "# Create train and test datasets \n",
    "# (X_train_circle--> arr_0, X_test_circle--> arr1_, Y_train_circle--> arr_2, Y_test_circle--> arr_3)\n",
    "X_train_circle = ???\n",
    "X_test_circle = ???\n",
    "Y_train_circle = ???\n",
    "Y_test_circle = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# Visualize non-linear features for both train and test data \n",
    "# Create two plots side-by-side (Use subplots)\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the data distribution what type of mapping do you suggest ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# Answer for the above question\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# Mapping R^2 to R^i\n",
    "\n",
    "X_test_circle_hat = ???\n",
    "X_train_circle_hat = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Train the model using GD for scenario 1 data\n",
    "\n",
    "Now that everything is ready, lets first train the model using scenario 1 data and evaluate model's test accuracy.\n",
    "\n",
    "Use predict and compute_loss_and_grad functions during training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "lr=1.0\n",
    "num_epochs = 30\n",
    "\n",
    "loss = np.zeros(num_epochs)\n",
    "theta = np.random.randn(X_train_circle.shape[1],1)\n",
    "    \n",
    "for ep in range(num_epochs):\n",
    "    # call predict function\n",
    "    Y_train_hat = ???\n",
    "    \n",
    "    # call the compute_loss_and_grad that you have implemented above to \n",
    "    # measure the loss and the gradient\n",
    "    loss[ep], grad_vec = ???\n",
    "\n",
    "    # update the theta parameter according to the GD here\n",
    "    theta =  ???\n",
    "\n",
    "# Obtain predicted class probabilities for train and test datasets\n",
    "Y_test_hat = ??? \n",
    "Y_train_hat = ??? \n",
    "\n",
    "# Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
    "Y_test_hat = ??? \n",
    "Y_train_hat = ??? \n",
    "\n",
    "# Evaluate the classification accuracy for training and test data\n",
    "test_score = ??? \n",
    "train_score = ??? \n",
    "\n",
    "print(\"Training accuracy: {:.3f}\".format(train_score))\n",
    "print(\"Test accuracy: {:.3f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation work correctly and using the provided hyperparameter settings, you should obtain something around: \n",
    "\n",
    "Training accuracy: 0.518 | Test accuracy: 0.487"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Decision Boundary for model trained with scenario 1 data\n",
    "\n",
    "Plot the decision boundary for the model trained on scenario 1 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "x_boundary_circle=np.linspace(-5,5,1000)\n",
    "# Predict decision boundary\n",
    "y_boundary_circle= ??? \n",
    "\n",
    "# Visualize ground truth data of testing dataset and the predicted decision boundary\n",
    "fig_circle=plt.figure()\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Train the model using GD for scenario 2 data\n",
    "\n",
    "Now, train the model using scenario 2 data and evaluate model's test accuracy.\n",
    "\n",
    "Again, use predict and compute_loss_and_grad functions during training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "lr=1.0\n",
    "num_epochs = 30\n",
    "\n",
    "loss = np.zeros(num_epochs)\n",
    "theta = np.random.randn(X_train_circle_hat.shape[1],1)\n",
    "    \n",
    "for ep in range(num_epochs):\n",
    "    # call predict function\n",
    "    Y_train_hat = ??? \n",
    "    \n",
    "    # call the compute_loss_and_grad that you have implemented above to \n",
    "    # measure the loss and the gradient\n",
    "    loss[ep], grad_vec = ??? \n",
    "\n",
    "    #update the theta parameter according to the GD here\n",
    "    theta =  ??? \n",
    "\n",
    "# Obtain predicted class probabilities for train and test datasets\n",
    "Y_test_hat = ??? \n",
    "Y_train_hat = ??? \n",
    "\n",
    "# Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
    "Y_test_hat = ??? \n",
    "Y_train_hat = ??? \n",
    "\n",
    "# Evaluate the classification accuracy for training and test data\n",
    "test_score = ??? \n",
    "train_score = ??? \n",
    "\n",
    "print(\"Training accuracy: {:.3f}\".format(train_score))\n",
    "print(\"Test accuracy: {:.3f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation work correctly and using the provided hyperparameter settings, you should obtain something around: \n",
    "\n",
    "Training accuracy: 0.912 | Test accuracy: 0.902"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Decision Boundary for model trained with scenario 2 data\n",
    "\n",
    "Plot the decision boundary for the model trained on scenario 2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "xx=np.linspace(-2,2,1000)\n",
    "yy=np.linspace(-2,2,1000)\n",
    "[X2,Y2]=np.meshgrid(xx,yy)\n",
    "\n",
    "# Reshape data according to the scenario 2 mapping and predict decision boundary\n",
    "???\n",
    "\n",
    "# Visualize ground truth data of testing dataset and the predicted decision boundary\n",
    "fig=plt.figure()\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Evaluate models performance with Decaying Learning rate \n",
    "\n",
    "Let's try to improve accuracy of the model trained on scenario 2 data using decaying learning rate during model training.\n",
    "\n",
    "In the previous tasks, you used a fixed learning rate during training. Can we improve model's accuracy by having a dynamic learning rate? Lets find out!\n",
    "\n",
    "Resources: https://www.mygreatlearning.com/blog/understanding-learning-rate-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "# Write a simple function to decrease current learning rate by 1% in each epoch\n",
    "def decaying_lr(lr):\n",
    "    new_lr = ??? \n",
    "    \n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='rgba(255,0,0,0.2)';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background(red_bgd)\n",
    "\n",
    "lr=1.0\n",
    "num_epochs = 30\n",
    "\n",
    "loss = np.zeros(num_epochs)\n",
    "theta = np.random.randn(X_train_circle_hat.shape[1],1)\n",
    "    \n",
    "for ep in range(num_epochs):\n",
    "    # call predict function\n",
    "    Y_train_hat = ??? \n",
    "    \n",
    "    # call the compute_loss_and_grad that you have implemented above to \n",
    "    # measure the loss and the gradient\n",
    "    loss[ep], grad_vec = ??? \n",
    "\n",
    "    # update the theta parameter according to the GD here\n",
    "    theta =  ??? \n",
    "    \n",
    "    # call decaying_lr function\n",
    "    lr = ???\n",
    "    \n",
    "# Obtain predicted class probabilities for train and test datasets\n",
    "Y_test_hat = ??? \n",
    "Y_train_hat = ??? \n",
    "\n",
    "# Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
    "Y_test_hat = ??? \n",
    "Y_train_hat = ??? \n",
    "\n",
    "# Evaluate the classification accuracy for training and test data\n",
    "test_score = ??? \n",
    "train_score = ??? \n",
    "\n",
    "print(\"Training accuracy: {:.3f}\".format(train_score))\n",
    "print(\"Test accuracy: {:.3f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation work correctly and using the provided hyperparameter settings, you should obtain something around: \n",
    "\n",
    "Training accuracy: 0.930 | Test accuracy: 0.920"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not remove or edit the following code snippet. \n",
    "\n",
    "When submitting your report, please ensure that you have run the entire notebook from top to bottom. You can do this by clicking \"Kernel\" and \"Restart Kernel and Run All Cells\". Make sure the last cell (below) has also been run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = str(student_number) + '_Lab2_Submission'\n",
    "cmd = \"jupyter nbconvert --to script Lab2_Student.ipynb --output \" + file_name\n",
    "if(os.system(cmd)):\n",
    "    print(\"Error converting to .py\")\n",
    "    print(\"cmd\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70931b1f795e9f6c4a65c63751d6e1a5ce2513e077f66f45128f19b7bedfe461"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
